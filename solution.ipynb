{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK planet Code Sprint 2015 Round 2 풀이\n",
    "\n",
    "## 들어가기\n",
    "\n",
    "* 이 내용의 작성자는 Jinhyun \"lewha0\" Kim이다(gmail 계정 lewha0).\n",
    "* 아래 내용은 SK planet Code Sprint 2015 Round 2에 대한 풀이를 담고 있다.\n",
    "* 설명된 내용과 실제 코드가 약간 다를 수 있다.\n",
    " * 이는 해당 내용을 구현할 때 구현상의 편의를 위하여 설명된 내용을 약간 변형한 것일 수도 있고, 역으로 이해를 돕기 위하여 구현된 내용을 단순화하여 설명하였을 수도 있다.\n",
    " * 또한 문서 작성 시점 이후에도 점수 개선을 위하여 코드를 약간 변형하였을 수 있다.\n",
    " * 혹은, 코드를 간결하게 바꾸는 과정에서 tie-breaking rule에 미묘한 차이가 생기면서 결과가 아주 약간 달라지는 경우도 있음을 발견하였다.\n",
    " * 그러나 큰 틀 안에서는 설명 내용은 계속 유효할 것이며, 큰 틀이 바뀌게 되면 설명 내용도 바뀔 것이다.\n",
    "* 설명된 내용은 수 차례의 실험을 통해 개선하였던 방법에 대한 것이며, 따라서 '왜 이와 같이 하였는가?'란 물음에 대한 답은 대개는 '실험해보니 좋아서'가 될 것이다. 가급적이면 내용을 설명할 때 사고 과정이 드러날 수 있도록 작성하였다.\n",
    "* **저는 python을 제대로 공부해 본 적이 없으며, 따라서 잘 쓸줄 모르고, 그렇기 때문에 코드가 못생겼을 것입니다. 이 점 너그러이 이해하고 봐 주실 것을 부탁드립니다. 이를테면, 저는 이번 기회를 통해서 zip이나 enumerate 같은 것들을 처음 사용해 보았는데, 아직도 이들이 어떠한 작업을 수행해 주는지는 알지 못합니다. 죄송합니다.**\n",
    "* 채점 기준이 대회 중간에 수정되었으나, 이로 인하여 의욕이 많이 상실되었기 때문에 대대적인 수정을 가하지는 못하였고, 일부만 수정하여 최종적으로 제출하게 되었다.\n",
    "\n",
    "## 기본 아이디어\n",
    "\n",
    "* 컨텐츠 사이의 유사도 sim(cx, cy)를 계산한다. 유사도는 0에서 1 사이의 값을 갖도록 한다.\n",
    "* 사용자 u가 cx라는 컨텐츠를 구매하였다면, 그 사용자의 cx라는 컨텐츠에 대한 점수 score(u, cx)를 sim(cx, cy) 만큼 증가시킨다. 이를 u가 구매한 모든 컨텐츠에 대해 적용한다.\n",
    "* 사용자 u에 대해서 가장 점수가 높은 영화 컨텐츠, 즉 score(u, c)를 최대로 하는 c(단 c는 영화)를 선택한다. 그리고 그 사용자에 대해서 c를 **100개** 출력한다!\n",
    "\n",
    "## 컨텐츠 사이의 유사도 계산\n",
    "\n",
    "* 각각의 컨텐츠에 대한 정보를 하나의 문자열로 만든다.\n",
    " \n",
    " * 먼저 텍스트 형태로 되어있는 정보를 추려내어 하나의 문자열을 만든다. *제목, 시놉시스, 감독명 배열, 출연자명 배열, 세부 장르, 제작 국가*가 이에 해당한다.\n",
    "  > 비엔나 호텔의 야간 배달부 1957년의 비엔나. 과거 칼펜부르노 (하략)\n",
    "  \n",
    " * 문자열에 포함된 내용 중, 학습에 방해가 될 것 같은 부분 문자열을 공백으로 치환한다. 예를 들어 괄호, 문장 부호, 하이픈 등이 있으면 이를 하나의 공백으로 치환한다.\n",
    " \n",
    " * 텍스트 정보가 더 잘 드러날 수 있도록, 원본 문자열의 모든 bigram(2-gram)을 문자열에 덧붙인다.\n",
    "  > 비엔나 호텔의 야간 배달부 1957년의 비엔나. 과거 칼펜부르노 (중략) 비엔 엔나 나호 호텔 텔의 (하략)\n",
    "\n",
    " * 원본 문자열의 모든 3-gram, 4-gram도 문자열에 덧붙인다.\n",
    "  > 비엔나 호텔의 야간 배달부 1957년의 비엔나. 과거 칼펜부르노 (중략) 비엔 엔나 나호 호텔 텔의 (중략) 비엔나 엔나호 나호텔 (중략) 비엔나호 엔나호텔 나호텔의 (하략)\n",
    "  \n",
    " * 이 문자열에 텍스트 형태가 아닌 정보를 이어붙인다. 이 때 각각의 정보가 어느 필드에 해당하는지를 표현할 수 있도록 정보 앞에 필드 번호를 붙인다. 이는 해당 정보가 텍스트 정보 안에 포함되어 있는 것과 구별하기 위함이다. 예를 들어, 영화가 1974년도에 나왔다면 시놉시스에도 1974가 등장할 수 있으므로 이 둘이 구별되도록 하기 위함이다. *필드 번호는 0부터 시작한다*.\n",
    " > 비엔나 호텔의 (중략) 2_FL 3_MOV 6_1974 7_19971108 13_18 14_14/07/11\n",
    "\n",
    " * 해당 컨텐츠를 구매한 사용자들의 목록도 이어붙인다. 만일 한 사용자가 여러 번 구매했다면 그만큼 여러 번 이어붙인다. 마찬가지로 해당 내용이 구매 사용자의 번호임을 알 수 있도록 하는 정보를 덧붙여 표현한다. *사용자 번호도 0부터 시작한다*.\n",
    " > 비엔나 호텔의 (중략) 2_FL 3_MOV 6_1974 7_19971108 13_18 14_14/07/11 (중략) user_0 user_0 user_1986 user_2222\n",
    "\n",
    "* 문자열을 벡터로 바꾼다.\n",
    "\n",
    " * TF-IDF를 사용한다.\n",
    " \n",
    " * 텍스트 형태가 아닌 정보, 혹은 구매한 사용자 정보 등은 어떻게 보면 해당 컨텐츠를 표현하고 있는 문서의 일부라기보다는 독자적인 정보일 것이다. 따라서 이를 한꺼번에 뭉뚱그려 TF-IDF를 계산하는 것이 일견 비합리적으로 보일 수도 있으며, 해당 정보에 대해서는 빈도수를 세는 등의 방식이 좋아보이기도 한다. 그러나 실험해보니 그냥 이렇게 TF-IDF를 사용하는게 제일 괜찮았다.\n",
    " \n",
    " * 이제 각각의 컨텐츠가 하나의 벡터로 표현되었다.\n",
    " \n",
    "* 컨텐츠 사이의 유사도를 계산한다.\n",
    "\n",
    " * 두 컨텐츠에 대한 벡터 두 개의 cosine similarity를 사용한다. TF-IDF 벡터는 sparse하게 표현될 것이므로 이를 빠르게 계산할 수 있기 때문이다.\n",
    " \n",
    " * 실제로 활용되는 것은 임의의 컨텐츠 cx와 영화 컨텐츠 cy 사이의 유사도이다. 따라서 유사도를 계산할 때에는 한 쪽은 모든 컨텐츠, 다른 한 쪽은 영화 컨텐츠로 하였다.\n",
    " \n",
    " * 자기 자신에 대한 유사도는 1로 표현될 것이다. **이를 따로 보정하지 않고 그대로 놔 둔다**. 여러 방법으로 보정을 해 보았는데 별로였다. **재구매 만세!**.\n",
    " \n",
    "## 실험\n",
    "\n",
    "* 주어진 데이터를 training data와 validation data로 나눠서 사용하였다.\n",
    " \n",
    "* 데이터를 나눌 때에는 가장 단순한 방법을 사용했는데, 1월까지의 데이터로 학습을 하여 2월 데이터에 대해 평가를 수행하였다.\n",
    " \n",
    "* 이와 같이 하게 되면 결과 수치가 조금 작게 나올 수 있는데, 이는 2월에 영화를 하나도 구매하지 않은 사용자가 있기 때문이다. 이러한 사용자는 애초에 없다고 생각하고 점수를 계산하였다. 즉, N을 줄여서 사용하였다.\n",
    " \n",
    "* Validation set에 대한 점수와 실제 제출하여 받는 점수 사이에은 **매우 유의미한** 상관관계가 있었다.\n",
    " \n",
    "* 최종적으로 제출할 때에는 주어진 전체 데이터를 사용한다.\n",
    " \n",
    "## 결과 출력\n",
    "\n",
    "* 과거 SK planet Code Sprint의 경험상, 점수 계산 방식만 잘 활용하여도 높은 추가점을 획들할 수 있는 경우가 몇 번 있었다. 이번에도 그렇지 않을까 하는 생각에 이런저런 방법을 시도해보게 되었고, 뒤늦게나마 한 방법을 찾았다.\n",
    " \n",
    "* MAP을 이리저리 분석해 보니 해당 사용자가 구매하였을 확률이 가장 높은 영화 하나만 100개 출력하는게 좋아 보였다. 실제로도 그러했다.\n",
    " \n",
    "* 이는 아직도 그 이유를 정확히 모르겠는데, 대략 두 가지 이유 때문인 것으로 판단된다.\n",
    " \n",
    " * 사용자는 실제로 그리 많은 영화를 구매하지 않는다. 91,223명의 사용자는 6개월간 5,978,953건의 컨텐츠를 구매하였다. 한 사람이 한 달에 평균 10번 컨텐츠를 구매하는 것이다. 영화는 더 적을 것이다. 이 때, 해당 사용자가 구매한 영화 중 하나만 제대로 맞힐 수 있다면, 그 사용자에 대해서 100/n 점을 얻을 수 있다. 실험을 해보니 이와 같이 영화 하나를 제대로 맞히는 것도 제법 수월한 편이었던 것으로 보이고, 각 사용자의 n이 작아서 점수가 뻥튀기되는 정도도 컸던 것으로 생각된다. 개인적인 견해로는, 이처럼 실제로 구매한 컨텐츠 수에 비해서 예측해야 하는 컨텐츠 수가 많은 경우라면 MAP이 부적절한 것 같다. 1만 번 영화를 구매하는 사람들에 대해서 100개를 예측하게 하는 것은 의미가 있겠으나, 이처럼 영화를 10번도 구매하지 않는 사용자라면 MAP이 다소 엉뚱한 결과를 내는 것은 아닌가 하는 생각이 든다.\n",
    "   \n",
    " * 사용자는 생각보다 자주 컨텐츠를 재구매한다. 본인의 Btv 사용 경험에 따르면(주어진 데이터가 Btv와 연관이 있을지는 모르겠으나), 무료 영화가 많이 올라와 있으며 따라서 생각날 때마다 이를 구매하여 시청할 수 있다. 꼭 그런 이유가 아니더라도 컨텐츠를 재구매하는 사용자도 제법 많았고, 재구매율이 높은 컨텐츠도 꽤 많았다. 그 덕분에 각 사용자가 구매할 영화 중 하나를 맞히는 것이 생각보다 수월했던 것으로 생각된다.\n",
    " \n",
    "## 최종 버전\n",
    "\n",
    "* 최종적으로는 중복 예측이 불허되었기 때문에 이 버전을 수정하여 사용하였다.\n",
    "\n",
    "* score가 제일 높은 100개의 영화를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기초 작업\n",
    "* 필요한 것들을 import 해 오고, 상수처럼 사용할 변수를 초기화하고, 시간 계산을 위한 기초 작업을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:42:25.931780 --- 0.000144958496094\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "train_filename = '../rsc/round2_purchaseRecord.tsv'\n",
    "user_n = 91223\n",
    "movie_n = 8106\n",
    "output_k = 100\n",
    "\n",
    "start_time = time.time()\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상품 정보 읽기\n",
    "* text_feature에 텍스트 정보를, other_feature에 텍스트가 아닌 정보를 문자열 형태로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:42:27.882359 --- 1.95072197914\n"
     ]
    }
   ],
   "source": [
    "text_feature = dict()\n",
    "other_feature = dict()\n",
    "\n",
    "with open('../rsc/round2_itemInfo.tsv', 'rb') as fin:\n",
    "    while True:\n",
    "        row = fin.readline().split('\\t')\n",
    "        if(len(row) <> 15): break\n",
    "\n",
    "        glss_id = row[0]\n",
    "        \n",
    "        for sth in [4, 5, 9, 10, 11, 12]:\n",
    "            if row[sth] <> '':\n",
    "                text_feature[glss_id] = text_feature.get(glss_id, '') + row[sth] + ' '\n",
    "\n",
    "        for sth in [1, 2, 3, 6, 7, 8, 13, 14]:\n",
    "            if row[sth] <> '':\n",
    "                other_feature[glss_id] = other_feature.get(glss_id, '') + str(sth) + '_' + row[sth] + ' '\n",
    "        \n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구매 내역 읽기\n",
    "* other_feature에 해당 컨텐츠를 구매한 사용자 명단을 덧붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:42:58.583130 --- 32.651499033\n"
     ]
    }
   ],
   "source": [
    "with open(train_filename, 'rb') as fin:\n",
    "    while True:\n",
    "        row = fin.readline().split('\\t')\n",
    "        if len(row) <> 4: break\n",
    "        \n",
    "        glss_id = row[2]\n",
    "        user_id = int(row[0]) - 1\n",
    "        \n",
    "        other_feature[glss_id] = other_feature.get(glss_id, '') + ' user_' + str(user_id) + ' '\n",
    "\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 정보 보강하기\n",
    "* text_feature에 k-gram들을 덧붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:43:07.718074 --- 41.7864358425\n"
     ]
    }
   ],
   "source": [
    "SPLITTER_RE = re.compile(\"( |\\\\\\|\\[|\\]|/|\\(|\\))\")\n",
    "\n",
    "for key in text_feature.keys():\n",
    "    curr_text = text_feature[key]\n",
    "    \n",
    "    for sth in ['\\n', '\\r', '[', ']', '(', ')', '+', '/', '\\\\', '.', ',', '!', '~', ':', ';', '\\t', '-', '_']:\n",
    "        curr_text = curr_text.replace(sth, ' ')\n",
    "    \n",
    "    word_list = [sth for sth in SPLITTER_RE.split(curr_text) if (sth <> '' and sth <> ' ')]\n",
    "    merged_text = ''.join(word_list).decode('utf-8')\n",
    "    \n",
    "    curr_text = ' '.join(word_list)\n",
    "    curr_text = curr_text + ' ' + ' '.join([''.join(sth) for sth in zip(*[merged_text[i:] for i in range(2)])]).encode('utf-8')\n",
    "    curr_text = curr_text + ' ' + ' '.join([''.join(sth) for sth in zip(*[merged_text[i:] for i in range(3)])]).encode('utf-8')\n",
    "    curr_text = curr_text + ' ' + ' '.join([''.join(sth) for sth in zip(*[merged_text[i:] for i in range(4)])]).encode('utf-8')\n",
    "\n",
    "    text_feature[key] = curr_text\n",
    "    \n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구매 내역 읽기\n",
    "* text_feature와 other_feature를 하나로 합친다.\n",
    "* 합친 내용을 all_features에 넣고, 영화의 경우에는 movie_features에도 넣는다.\n",
    "* 결과 재현이 잘 안 되어서 컨텐츠의 순서를 랜덤하게 섞어 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:43:07.911707 --- 41.9800829887\n"
     ]
    }
   ],
   "source": [
    "all_features = list()\n",
    "movie_features = list()\n",
    "movie_ids = list()\n",
    "contents_index = dict()\n",
    "\n",
    "all_keys = text_feature.keys()\n",
    "random.shuffle(all_keys)\n",
    "\n",
    "for key in all_keys:\n",
    "    final_feature =  text_feature[key] + ' ' + other_feature[key]\n",
    "    \n",
    "    contents_index[key] = len(all_features)\n",
    "    all_features.append(final_feature)\n",
    "    \n",
    "    if '3_MOV' in other_feature[key]:\n",
    "        movie_features.append(final_feature)\n",
    "        movie_ids.append(key)\n",
    "\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 만들기 및 유사도 계산하기\n",
    "* 데이터를 TfidfVectorizer를 이용하여 TF-IDF 벡터로 만든다\n",
    "* 전체 컨텐츠에 대한 벡터 all_vector와 영화 컨텐츠에 대한 벡터 movie_vector를 만든다.\n",
    "* 이 두 벡터 사이의 유사도를 계산한다. 유사도는 cosine_similarity로 계산하고, sim_matrix라는 배열에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:43:08.026619 --- 42.0949718952\n",
      "2015-07-23 15:44:08.897389 --- 102.966188908\n",
      "2015-07-23 15:44:17.752697 --- 111.821465969\n",
      "2015-07-23 15:44:43.240911 --- 137.309717894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "all_vector = vectorizer.fit_transform(all_features)\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "movie_vector = vectorizer.transform(movie_features)\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(all_vector, movie_vector)\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수 계산하기\n",
    "* 각 사용자의 각 영화에 대한 점수를 계산한다.\n",
    "* 구매 내역을 다시 살펴보면서, 어떤 사용자가 어떤 컨텐츠를 구매하였다면, 그 컨텐츠와 영화 사이의 유사도를 누적시켜서 점수를 계산한다.\n",
    "* 최종적으로 점수는 score 배열에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:44:43.249212 --- 137.317547798\n",
      "2015-07-23 15:44:44.391963 --- 138.46071887\n",
      "2015-07-23 15:45:40.070143 --- 194.138865948\n"
     ]
    }
   ],
   "source": [
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "score = [numpy.zeros(movie_n, dtype='float64') for _ in xrange(user_n)]\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "\n",
    "with open(train_filename, 'rb') as fin:\n",
    "    while True:\n",
    "        row = fin.readline().split('\\t')\n",
    "        if len(row) <> 4: break\n",
    "        \n",
    "        glss_id = row[2]\n",
    "        user_id = int(row[0]) - 1\n",
    "        \n",
    "        index = contents_index[glss_id]\n",
    "        score[user_id] += sim_matrix[index]\n",
    "\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 구하기 및 출력하기\n",
    "* 각 사용자별로 최대 점수를 주는 영화를 먼저 찾고, 이 영화를 100번 출력한다.\n",
    "* 만일 최대 점수의 영화가 여럿 있다면 그 중의 하나를 랜덤하게 선택한다.\n",
    "* 가 아니고, 정렬된 순서대로 score가 높은 영화를 100개 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:45:40.075822 --- 194.144156933\n",
      "2015-07-23 15:46:29.799320 --- 243.8680439\n"
     ]
    }
   ],
   "source": [
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)\n",
    "\n",
    "with open('predict.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    for user_id in xrange(user_n):\n",
    "        best_movies = score[user_id].argsort()[::-1][:output_k]\n",
    "        for best_movie_index in best_movies:\n",
    "            writer.writerow([user_id + 1, movie_ids[best_movie_index]])\n",
    "\n",
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-23 15:46:29.802487 --- 243.87082386\n"
     ]
    }
   ],
   "source": [
    "print str(datetime.datetime.now()), \"---\", (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
